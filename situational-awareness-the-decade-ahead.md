# Situational Awareness - The Decade Ahead

Author: Leopold Ashenbrenner
Link: [situationalawareness.pdf](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf)

To be fair, I think that Ray Kurzweil had a lot of the same ideas - ie. we would create AGI first and then it would bootstrap itself to ASI and at the point we reach ASI this is what Ray called the "Singularity"

It is absolutely insane when you consider how far AI has come since 2019. I remember using GPT-2 and it was weird and a toy. Now we have GPT-4 and entire businesses (including the one that employes me) are possible because of it.

When we look back at what GPT-2 could do and then we compare it to GPT-4, it's quite a stark jump in capability. Some are saying we won't see that jump again because we haven't gotten a new SOTA model since GPT-4 which finished training in 2022. But Leopold is saying that we will of course continue to make these exponential leaps in compute, algorithmic and "unhobbling" progress and could have a PhD level AI by 2027.

This manifesto really puts the progress in perspective. It's so hard to see how far you've come when you're still enamored with the fact that you're moving at mach 10.

I don't get how Leopold's paper is "terrifying"

He's basically saying that the people who are explicitly trying to develop AGI are going to succeed relatively soon

Which is the point of the whole thing isn't it? https://x.com/DannyMcAteer8/status/1801791141001449486

LLMs just breeze through data without giving it any second thought. They don't have any depth to their thought. How can we add depth?

Once have broken the code for AGI, it won't be that well have just one AGI - we'll be able to deploy a legion of hundreds of millions of AGIs that can do high level cognitive work day and night

The idea of intelligence explosion is similar to Nick Bostrom's "Superintelligence" hypothesis of an intelligence explosion, but as far as I know Bostrom doesn't focus on specifically deploying AI researchers who are in fact researching AI techniques

"Expect 100 million automated researchers each working at 100x human speed not long after we begin to be able to automate AI research."

We're literally taking the latent capabilities of the physical universe and using them to create intelligence

It seems like the algorithmic improvements are advancing the slowest of Leopold's three pillars - does that mean that's the area for most improvement? Or does it mean we should focus on the other areas?

We haven't even gotten out of the first inning of optimizing the shit out of #LLMs , there's still so much low-hanging fruit

2024-06-21 - Anthropic release Claude Sonnet 3.5 yesterday. It seems to be significantly stronger than GPT-4o. What are the possible algorithmic improvements or unhobbling they could have done to make it that much better?

It's insane that human beings can think of abstract ways of moving information around in computers that haven't been thought of before and that can lead to changing the world forever, and there's still more to be discovered

We may be developing the algorithmic techniques necessary to get to AGI right now, and if we don't protect them it would be very easy for the Chinese to steal them

If the InfoSec stance at OpenAI is becomeing much more strict and people are being monitored, this could explain why people like Karpathy and Ilya left. Those guys seem to like the open science and if Leopold is right then things are already way less open and going to only get more so.

The worst possible outcome is a neck and neck race to SI, where the US and China are racing ahead trying to beat each other, and there's no room left for considering safety or alignment.
