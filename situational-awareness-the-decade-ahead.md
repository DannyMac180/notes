# Situational Awareness - The Decade Ahead

Author: Leopold Ashenbrenner
Link: [situationalawareness.pdf](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf)

To be fair, I think that Ray Kurzweil had a lot of the same ideas - ie. we would create AGI first and then it would bootstrap itself to ASI and at the point we reach ASI this is what Ray called the "Singularity"

It is absolutely insane when you consider how far AI has come since 2019. I remember using GPT-2 and it was weird and a toy. Now we have GPT-4 and entire businesses (including the one that employes me) are possible because of it.

When we look back at what GPT-2 could do and then we compare it to GPT-4, it's quite a stark jump in capability. Some are saying we won't see that jump again because we haven't gotten a new SOTA model since GPT-4 which finished training in 2022. But Leopold is saying that we will of course continue to make these exponential leaps in compute, algorithmic and "unhobbling" progress and could have a PhD level AI by 2027.

This manifesto really puts the progress in perspective. It's so hard to see how far you've come when you're still enamored with the fact that you're moving at mach 10.

I don't get how Leopold's paper is "terrifying"

He's basically saying that the people who are explicitly trying to develop AGI are going to succeed relatively soon

Which is the point of the whole thing isn't it? https://x.com/DannyMcAteer8/status/1801791141001449486

LLMs just breeze through data without giving it any second thought. They don't have any depth to their thought. How can we add depth?

Once have broken the code for AGI, it won't be that well have just one AGI - we'll be able to deploy a legion of hundreds of millions of AGIs that can do high level cognitive work day and night

The idea of intelligence explosion is similar to Nick Bostrom's "Superintelligence" hypothesis of an intelligence explosion, but as far as I know Bostrom doesn't focus on specifically deploying AI researchers who are in fact researching AI techniques

"Expect 100 million automated researchers each working at 100x human speed not long after we begin to be able to automate AI research."

It seems like the algorithmic improvements are advancing the slowest of Leopold's three pillars - does that mean that's the area for most improvement? Or does it mean we should focus on the other areas?

We haven't even gotten out of the first inning of optimizing the shit out of #LLMs , there's still so much low-hanging fruit
